{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the global statistics of the predictions when we train different models from a cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class names...\n",
      "Loading class names...\n",
      "Loading class names...\n",
      "Loading class names...\n",
      "Loading class names...\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/metrics-and-python-ii-2e49597964ff\n",
    "# https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "# en el apartado \"a worked example\" de wikipedia viene muy bien explicado\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from imgclas.data_utils import load_image, load_class_names\n",
    "from imgclas import paths, plot_utils\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from imgclas import test_utils\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # To ignore UndefinedMetricWarning: [Recall/Precision/F-Score] is ill-defined and being set to 0.0 in labels with no [true/predicted] samples.\n",
    "\n",
    "# User parameters to set\n",
    "timestamp = ['2022-04-19_Fold1SpAnd45Balanced_18ep_stop_16Batch', \n",
    "             '2022-04-19_Fold2SpAnd45Balanced_17ep_stop_16Batch',\n",
    "            '2022-04-19_Fold3SpAn45Balanced_12ep_stop_16Batch', \n",
    "             '2022-04-20_Fold4SpAnd45Balanced_35ep_stop15_16Batch',\n",
    "            '2022-04-20_Fold5SpAnd45Balanced_21ep_stop15_16Batch']      # timestamp of the model\n",
    "SPLIT_NAME = 'test'                   # dataset split to predict\n",
    "MODEL_NAME = 'final_model.h5'         # model to use to make the mediction\n",
    "TOP_K = 2                             # number of top classes predictions to save\n",
    "\n",
    "accs = []\n",
    "sens = []\n",
    "specs = []\n",
    "ppv=[]\n",
    "npv=[]\n",
    "aucs=[]\n",
    "accuracy=[]\n",
    "f1_scores=[]\n",
    "f1_scores_sklearn=[]\n",
    "prevalences=[]\n",
    "for TIMESTAMP in timestamp:\n",
    "\n",
    "    # Set the timestamp\n",
    "    paths.timestamp = TIMESTAMP\n",
    "\n",
    "    # Load clas names\n",
    "    class_names = load_class_names(splits_dir=paths.get_ts_splits_dir())\n",
    "\n",
    "    # Load back the predictions\n",
    "    pred_path = os.path.join(paths.get_predictions_dir(), '{}+{}+top{}.json'.format(MODEL_NAME, SPLIT_NAME, TOP_K))\n",
    "    with open(pred_path) as f:\n",
    "        pred_dict = json.load(f)\n",
    "    \n",
    "    # accuracy\n",
    "    true_lab, pred_lab = np.array(pred_dict['true_lab']), np.array(pred_dict['pred_lab'])\n",
    "    top1 = test_utils.topK_accuracy(true_lab, pred_lab, K=1)\n",
    "    accs.append(top1)\n",
    "    \n",
    "    y_pred = np.array([item[0] for item in pred_lab])\n",
    "    # standard confussion matrix\n",
    "    TN, FP, FN, TP = confusion_matrix(true_lab, y_pred, labels=[0, 1]).ravel()\n",
    "    Population = TN+FN+TP+FP\n",
    "    sensitivity  = round( TP / (TP+FN),4 ) # recall\n",
    "    specificity  = round( TN / (TN+FP),4 ) \n",
    "    pos_pred_val = round( TP / (TP+FP),4 ) # precision\n",
    "    neg_pred_val = round( TN / (TN+FN),4 )\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    F1 = round ( 2 * ((pos_pred_val*sensitivity)/(pos_pred_val+sensitivity)),4)\n",
    "    accuracy.append(Accuracy)\n",
    "    ppv.append(pos_pred_val)\n",
    "    npv.append(neg_pred_val)\n",
    "    sens.append(sensitivity)\n",
    "    specs.append(specificity)\n",
    "    f1_scores.append(F1)\n",
    "    prevalences.append(Prevalence)\n",
    "    f1_scores_sklearn.append(f1_score(true_lab, pred_lab[:, 0]))\n",
    "    \n",
    "    scores=[]\n",
    "    for i in range(0, len(pred_dict['pred_lab'])):\n",
    "        if pred_dict['pred_lab'][i][0]==0:\n",
    "            scores.append(pred_dict['pred_prob'][i][1])\n",
    "        else:\n",
    "            scores.append(pred_dict['pred_prob'][i][0])\n",
    "    auc = roc_auc_score(true_lab, scores)\n",
    "    aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prevalences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy is 0.87 and its mean SD is 0.06\n"
     ]
    }
   ],
   "source": [
    "mean_accs=np.mean(accs)\n",
    "sd_accs=np.std(accs)\n",
    "print('Mean accuracy is {:.2f} and its mean SD is {:.2f}'.format(mean_accs,sd_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean specificity is 0.88 and its mean SD is 0.10\n"
     ]
    }
   ],
   "source": [
    "mean_specs=np.mean(specs)\n",
    "sd_specs=np.std(specs)\n",
    "print('Mean specificity is {:.2f} and its mean SD is {:.2f}'.format(mean_specs,sd_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sensitivity is 0.87 and its mean SD is 0.10\n"
     ]
    }
   ],
   "source": [
    "mean_sens=np.mean(sens)\n",
    "sd_sens=np.std(sens)\n",
    "print('Mean sensitivity is {:.2f} and its mean SD is {:.2f}'.format(mean_sens,sd_sens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean PPV is 0.89 and its mean SD is 0.08\n"
     ]
    }
   ],
   "source": [
    "mean_ppv=np.mean(ppv)\n",
    "sd_ppv=np.std(ppv)\n",
    "print('Mean PPV is {:.2f} and its mean SD is {:.2f}'.format(mean_ppv,sd_ppv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NPV is 0.88 and its mean SD is 0.09\n"
     ]
    }
   ],
   "source": [
    "mean_npv=np.mean(npv)\n",
    "sd_npv=np.std(npv)\n",
    "print('Mean NPV is {:.2f} and its mean SD is {:.2f}'.format(mean_npv,sd_npv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC is 0.96 and its mean SD is 0.03\n"
     ]
    }
   ],
   "source": [
    "mean_auc=np.mean(aucs)\n",
    "sd_auc=np.std(aucs)\n",
    "print('Mean AUC is {:.2f} and its mean SD is {:.2f}'.format(mean_auc,sd_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC is 0.87 and its mean SD is 0.07\n"
     ]
    }
   ],
   "source": [
    "mean_f1_scores=np.mean(f1_scores)\n",
    "sd_f1_scores=np.std(f1_scores)\n",
    "print('Mean F1 score is {:.2f} and its mean SD is {:.2f}'.format(mean_f1_scores,\n",
    "                                                            sd_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Prevalence is 0.49 and its mean SD is 0.08\n"
     ]
    }
   ],
   "source": [
    "mean_prevalences=np.mean(prevalences)\n",
    "sd_prevalences=np.std(prevalences)\n",
    "print('Mean Prevalence is {:.2f} and its mean SD is {:.2f}'.format(mean_prevalences,\n",
    "                                                            sd_prevalences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
